##

Collection of reproducible commands and sample logs for vLLM inference optimization blog series

- [Part 1: Optimizing LLM inference: A practical guide with vLLM, Quantization and Google Cloud GPUs](https://medium.com/@injae.kwak/part-1-optimizing-llm-inference-a-practical-guide-with-vllm-quantization-and-google-cloud-gpus-tpus-30521202d15b)
- [Part 2: Optimizing LLM inference: Speculative decoding and quantization on vLLM with Google Cloud GPUs](https://medium.com/@injae.kwak/part-2-optimizing-llm-inference-speculative-decoding-and-quantization-on-vllm-with-google-cloud-54b91f018496)
- [Part 3: Optimizing LLM inference: KV cache offloading on vLLM with Google Cloud TPUs]()
- [Part 4: Optimizing LLM inference: Optimizing vLLM inference with Google Cloud TPUs]()
- [Part 5: Optimizing LLM inference: Disaggregated Serving with llm-d and GKE inference gateway]()
