# VLLM Server with long context window (BF16 example)
VLLM_ATTENTION_BACKEND=FLASHINFER vllm serve "Qwen/Qwen3-8B" \
 --max-model-len 32768 --tensor-parallel-size 1 \
 --gpu-memory-utilization 0.9 --seed 42 &> serve.log &

# LM-Eval Client (MMLU-Pro example)
HF_ALLOW_CODE_EVAL=1 lm_eval --model local-completions \
 --tasks mmlu_pro --batch_size 1 \
 --model_args model=Qwen/Qwen3-8B,base_url=http://localhost:8000/v1/completions,max_length=32768,num_concurrent=64 \
 --seed 42 --show_config --confirm_run_unsafe_code --log_samples \
 --output_path ./Qwen3-8B_bf16.json --num_fewshot 5 --fewshot_as_multiturn \
 --apply_chat_template --gen_kwargs temperature=0.6,top_p=0.95,top_k=20,max_gen_toks=16384 \
 --limit 40 &> mmlu_bf16.log &

local-completions (model=Qwen/Qwen3-8B,base_url=http://localhost:8000/v1/completions,max_length=32768,num_concurrent=64), gen_kwargs: (temperature=0.6,top_p=0.95,top_k=20,max_gen_toks=16384), limit: 40.0, num_fewshot: 5, batch_size: 1
|       Tasks       |Version|    Filter    |n-shot|  Metric   |   |Value |   |Stderr|
|-------------------|------:|--------------|-----:|-----------|---|-----:|---|-----:|
|mmlu_pro           |    2.0|custom-extract|      |exact_match|↑  |0.6982|±  |0.0185|
| - biology         |    2.1|custom-extract|     5|exact_match|↑  |0.7750|±  |0.0669|
| - business        |    2.1|custom-extract|     5|exact_match|↑  |0.7500|±  |0.0693|
| - chemistry       |    2.1|custom-extract|     5|exact_match|↑  |0.9000|±  |0.0480|
| - computer_science|    2.1|custom-extract|     5|exact_match|↑  |0.7250|±  |0.0715|
| - economics       |    2.1|custom-extract|     5|exact_match|↑  |0.7500|±  |0.0693|
| - engineering     |    2.1|custom-extract|     5|exact_match|↑  |0.7250|±  |0.0715|
| - health          |    2.1|custom-extract|     5|exact_match|↑  |0.6750|±  |0.0750|
| - history         |    2.1|custom-extract|     5|exact_match|↑  |0.4750|±  |0.0800|
| - law             |    2.1|custom-extract|     5|exact_match|↑  |0.4500|±  |0.0797|
| - math            |    2.1|custom-extract|     5|exact_match|↑  |0.9000|±  |0.0480|
| - other           |    2.1|custom-extract|     5|exact_match|↑  |0.4250|±  |0.0792|
| - philosophy      |    2.1|custom-extract|     5|exact_match|↑  |0.5750|±  |0.0792|
| - physics         |    2.1|custom-extract|     5|exact_match|↑  |0.9000|±  |0.0480|
| - psychology      |    2.1|custom-extract|     5|exact_match|↑  |0.7500|±  |0.0693|

| Groups |Version|    Filter    |n-shot|  Metric   |   |Value |   |Stderr|
|--------|------:|--------------|------|-----------|---|-----:|---|-----:|
|mmlu_pro|      2|custom-extract|      |exact_match|↑  |0.6982|±  |0.0185|

# LM-Eval Client (BBH & GPQA example)
HF_ALLOW_CODE_EVAL=1 lm_eval --model local-completions \
 --tasks bbh_cot_fewshot,leaderboard_gpqa --batch_size 1 \
 --model_args model=Qwen/Qwen3-8B,base_url=http://localhost:8000/v1/completions,max_length=32768,num_concurrent=64 \
 --seed 42 --show_config --confirm_run_unsafe_code --log_samples \
 --output_path ./Qwen3-8B_bf16.json --num_fewshot 3 \
 --gen_kwargs temperature=0.6,top_p=0.95,top_k=20,max_gen_toks=16384 \
 --limit 40 &> bbh_gpqa_bf16.log &

local-completions (model=Qwen/Qwen3-8B,base_url=http://localhost:8000/v1/completions,max_length=32768,num_concurrent=64), gen_kwargs: (temperature=0.6,top_p=0.95,top_k=20,max_gen_toks=16384), limit: 40.0, num_fewshot: 3, batch_size: 1
|                          Tasks                           |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|----------------------------------------------------------|------:|----------|-----:|-----------|---|-----:|---|-----:|
|bbh_cot_fewshot                                           |      3|get-answer|      |exact_match|↑  |0.7935|±  |0.0113|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer|     3|exact_match|↑  |0.9750|±  |0.0250|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer|     3|exact_match|↑  |0.5500|±  |0.0797|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer|     3|exact_match|↑  |0.7500|±  |0.0693|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer|     3|exact_match|↑  |0.6000|±  |0.0784|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer|     3|exact_match|↑  |0.5000|±  |0.0801|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer|     3|exact_match|↑  |0.5000|±  |0.0801|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer|     3|exact_match|↑  |0.5750|±  |0.0792|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer|     3|exact_match|↑  |0.9500|±  |0.0349|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer|     3|exact_match|↑  |0.7500|±  |0.0693|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer|     3|exact_match|↑  |0.4750|±  |0.0800|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer|     3|exact_match|↑  |0.9500|±  |0.0349|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer|     3|exact_match|↑  |0.6750|±  |0.0750|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer|     3|exact_match|↑  |1.0000|±  |0.0000|
| - bbh_cot_fewshot_navigate                               |      4|get-answer|     3|exact_match|↑  |0.9750|±  |0.0250|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer|     3|exact_match|↑  |0.9000|±  |0.0480|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer|     3|exact_match|↑  |0.8500|±  |0.0572|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer|     3|exact_match|↑  |0.9250|±  |0.0422|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer|     3|exact_match|↑  |0.8250|±  |0.0608|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer|     3|exact_match|↑  |0.6500|±  |0.0764|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer|     3|exact_match|↑  |0.6750|±  |0.0750|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer|     3|exact_match|↑  |0.8500|±  |0.0572|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer|     3|exact_match|↑  |0.9000|±  |0.0480|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer|     3|exact_match|↑  |0.9750|±  |0.0250|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer|     3|exact_match|↑  |0.9500|±  |0.0349|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer|     3|exact_match|↑  |0.9750|±  |0.0250|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer|     3|exact_match|↑  |1.0000|±  |0.0000|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer|     3|exact_match|↑  |0.7250|±  |0.0715|
|leaderboard_gpqa                                          |       |none      |      |acc_norm   |↑  |0.4167|±  |0.0447|
| - leaderboard_gpqa_diamond                               |      1|none      |     0|acc_norm   |↑  |0.3750|±  |0.0775|
| - leaderboard_gpqa_extended                              |      1|none      |     0|acc_norm   |↑  |0.3250|±  |0.0750|
| - leaderboard_gpqa_main                                  |      1|none      |     0|acc_norm   |↑  |0.5500|±  |0.0797|

|     Groups     |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|----------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh_cot_fewshot |      3|get-answer|      |exact_match|↑  |0.7935|±  |0.0113|
|leaderboard_gpqa|       |none      |      |acc_norm   |↑  |0.4167|±  |0.0447|