# VLLM Server
VLLM_ATTENTION_BACKEND=FLASH_ATTN vllm serve "Qwen/Qwen3-8B-FP8" \
 --kv-cache-dtype fp8 --max-model-len 8192 --tensor-parallel-size 1 \
 --gpu-memory-utilization 0.9 --seed 42 &> serve.log &

# Guidellm Client (Prefill-heavy)
guidellm benchmark --target http://localhost:8000 --model "Qwen/Qwen3-8B-FP8" \
 --data 'source=benchmarks/sonnet.txt,prompt_tokens=2048,output_tokens=128,samples=300' \
 --rate-type throughput

Benchmarks Metadata:
    Run id:c0e3a84a-abd6-4c9a-a6ee-ac2a90c2790d
    Duration:23.1 seconds
    Profile:type=throughput, strategies=['throughput'], max_concurrency=None
    Args:max_number=300, max_duration=None, warmup_number=None, warmup_duration=None, cooldown_number=None, cooldown_duration=None
    Worker:type_='generative_requests_worker' backend_type='openai_http' backend_target='http://localhost:8000' backend_model='Qwen/Qwen3-8B-FP8'                      
    backend_info={'max_output_tokens': 16384, 'timeout': 300, 'http2': True, 'follow_redirects': True, 'headers': {}, 'text_completions_path': '/v1/completions',      
    'chat_completions_path': '/v1/chat/completions'}                                                                                                                   
    Request Loader:type_='generative_request_loader' data='source=benchmarks/sonnet.txt,prompt_tokens=2048,output_tokens=128,samples=300' data_args=None               
    processor='Qwen/Qwen3-8B-FP8' processor_args=None                                                                                                                  
    Extras:None
                                              
Benchmarks Stats:
======================================================================================================================================================
Metadata  | Request Stats         || Out Tok/sec| Tot Tok/sec| Req Latency (sec) ||| TTFT (ms)            ||| ITL (ms)         ||| TPOT (ms)        ||
 Benchmark| Per Second| Concurrency|        mean|        mean|  mean| median|   p99|   mean| median|     p99| mean| median|   p99| mean| median|   p99
----------|-----------|------------|------------|------------|------|-------|------|-------|-------|--------|-----|-------|------|-----|-------|------
throughput|      16.78|      287.56|      2147.6|     36512.8| 17.14|  17.22| 17.83| 6948.5| 6785.8| 13992.9| 80.2|   82.2| 124.2| 79.6|   81.5| 123.3
======================================================================================================================================================

# Guidellm Client (Balanced)
guidellm benchmark --target http://localhost:8000 --model "Qwen/Qwen3-8B-FP8" \
 --data 'source=benchmarks/sonnet.txt,prompt_tokens=1024,output_tokens=1024,samples=300' \
 --rate-type throughput

Benchmarks Metadata:
    Run id:6f2ddf74-9628-4bc3-992d-8ef9dc958998
    Duration:46.9 seconds
    Profile:type=throughput, strategies=['throughput'], max_concurrency=None
    Args:max_number=300, max_duration=None, warmup_number=None, warmup_duration=None, cooldown_number=None, cooldown_duration=None
    Worker:type_='generative_requests_worker' backend_type='openai_http' backend_target='http://localhost:8000' backend_model='Qwen/Qwen3-8B-FP8'                      
    backend_info={'max_output_tokens': 16384, 'timeout': 300, 'http2': True, 'follow_redirects': True, 'headers': {}, 'text_completions_path': '/v1/completions',      
    'chat_completions_path': '/v1/chat/completions'}                                                                                                                   
    Request Loader:type_='generative_request_loader' data='source=benchmarks/sonnet.txt,prompt_tokens=1024,output_tokens=1024,samples=300' data_args=None              
    processor='Qwen/Qwen3-8B-FP8' processor_args=None                                                                                                                  
    Extras:None
                                 
Benchmarks Stats:
===================================================================================================================================================
Metadata  | Request Stats         || Out Tok/sec| Tot Tok/sec| Req Latency (sec) ||| TTFT (ms)           ||| ITL (ms)        ||| TPOT (ms)       ||
 Benchmark| Per Second| Concurrency|        mean|        mean|  mean| median|   p99|   mean| median|    p99| mean| median|  p99| mean| median|  p99
----------|-----------|------------|------------|------------|------|-------|------|-------|-------|-------|-----|-------|-----|-----|-------|-----
throughput|       7.21|      297.29|      7385.2|     14775.8| 41.22|  41.26| 41.56| 3515.8| 3416.1| 6794.7| 36.9|   37.0| 39.5| 36.8|   37.0| 39.4
===================================================================================================================================================

# Guidellm Client (Decode-heavy)
guidellm benchmark --target http://localhost:8000 --model "Qwen/Qwen3-8B-FP8" \
 --data 'source=benchmarks/sonnet.txt,prompt_tokens=128,output_tokens=2048,samples=300' \
 --rate-type throughput

Benchmarks Metadata:
    Run id:2fb6f40b-aa45-462d-b93f-e90eebc1c99d
    Duration:75.4 seconds
    Profile:type=throughput, strategies=['throughput'], max_concurrency=None
    Args:max_number=300, max_duration=None, warmup_number=None, warmup_duration=None, cooldown_number=None, cooldown_duration=None
    Worker:type_='generative_requests_worker' backend_type='openai_http' backend_target='http://localhost:8000' backend_model='Qwen/Qwen3-8B-FP8' backend_info={'max_output_tokens': 16384, 'timeout': 300, 'http2': True, 'follow_redirects': True, 'headers': {}, 'text_completions_path': '/v1/completions',        
    'chat_completions_path': '/v1/chat/completions'}                                                                                                                                                                                                                                                                   
    Request Loader:type_='generative_request_loader' data='source=benchmarks/sonnet.txt,prompt_tokens=128,output_tokens=2048,samples=300' data_args=None processor='Qwen/Qwen3-8B-FP8' processor_args=None
    Extras:None

Benchmarks Stats:
=================================================================================================================================================
Metadata  | Request Stats         || Out Tok/sec| Tot Tok/sec| Req Latency (sec) ||| TTFT (ms)         ||| ITL (ms)        ||| TPOT (ms)       ||
 Benchmark| Per Second| Concurrency|        mean|        mean|  mean| median|   p99|  mean| median|   p99| mean| median|  p99| mean| median|  p99
----------|-----------|------------|------------|------------|------|-------|------|------|-------|------|-----|-------|-----|-----|-------|-----
throughput|       4.31|      299.52|      8828.5|      9381.2| 69.48|  69.49| 69.56| 617.7|  572.1| 960.8| 33.6|   33.7| 33.8| 33.6|   33.6| 33.8
=================================================================================================================================================